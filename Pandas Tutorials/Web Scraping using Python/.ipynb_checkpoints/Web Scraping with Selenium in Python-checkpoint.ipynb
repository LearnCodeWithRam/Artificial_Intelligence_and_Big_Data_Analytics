{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ff791e-4f15-4ccd-89b2-887bf474ce29",
   "metadata": {},
   "source": [
    "# Selenium Web Scraping\n",
    "\n",
    "Selenium web scraping involves using the browser automation tool to extract data from websites. Selenium allows you to programmatically control a web browser, meaning you can interact with websites just like a human user would. While various tools and libraries are available for web scraping in Python, Selenium stands out as a robust option, especially when dealing with websites that rely heavily on JavaScript for dynamic content rendering.\n",
    "\n",
    "## Overview of Selenium Web Scraping\n",
    "\n",
    "1. **Setup**: Install Selenium and a web driver (like ChromeDriver for Chrome or GeckoDriver for Firefox).\n",
    "2. **Automation**: Use Selenium to open a web browser and navigate to the target website.\n",
    "3. **Interaction**: Perform actions like clicking buttons, filling out forms, and scrolling, to interact with the web page.\n",
    "4. **Data Extraction**: Extract the desired data from the web page’s HTML content.\n",
    "5. **Processing**: Process and store the extracted data as needed.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, ensure that you have the following installed:\n",
    "\n",
    "1. **Python**: Download the latest version of Python from the official website.\n",
    "2. **Chrome Browser**: Selenium works best with the Chrome browser, so make sure you have it installed on your machine.\n",
    "3. **ChromeDriver**: ChromeDriver is essential for Selenium to control the Chrome browser. You can download it from the official website and ensure that the version matches your installed Chrome browser.\n",
    "4. **Selenium Library**: Install Selenium using pip with the following command:\n",
    "    \n",
    "   ` pip install selenium`\n",
    "    \n",
    "\n",
    "You also need to have:\n",
    "- Basic Python knowledge\n",
    "- Basic HTML\n",
    "- An open mind to learn new things\n",
    "\n",
    "## Getting Started with Selenium\n",
    "\n",
    "Let’s start with a basic example to understand how Selenium works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c8a41a-f7d4-4d89-8deb-18b0d5cb648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->webdriver-manager) (2.1.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6865d85-2027-42c9-9438-eb387aa1622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall webdriver-manager\n",
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c27e4-f424-47c0-9405-93c7127f3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv myenv\n",
    "myenv\\Scripts\\activate\n",
    "pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a89d6bd-c453-4750-aed8-d8b322d8b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\n",
      "Page Title: Neo4j documentation - Neo4j Documentation\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize ChromeDriver with automatic open chrome browser installed in your pc\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a website\n",
    "driver.get('https://neo4j.com/docs/')\n",
    "\n",
    "# Extract the page title\n",
    "page_title = driver.title\n",
    "print(\"Page Title:\", page_title)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec261cc-8199-45c4-b4fc-c218a655a034",
   "metadata": {},
   "source": [
    "In the above code, we imported the ‘webdriver’ module from Selenium, initialized the ChromeDriver, opened a website (https://www.example.com), extracted its page title, and then closed the browser using the ‘quit()’ method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef79381-899e-44d2-a19e-c3338dd1b372",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium\n",
    "\n",
    "Now that we have a basic understanding of Selenium, let’s explore more advanced web scraping concepts. Many websites load data dynamically using JavaScript, which makes standard libraries like `requests` and `beautifulsoup` inadequate. Selenium’s ability to interact with JavaScript-rendered content makes it a powerful choice for such scenarios.\n",
    "\n",
    "## Locating Elements\n",
    "\n",
    "To scrape data effectively, we need to locate elements on the web page. Elements can be located using various methods:\n",
    "\n",
    "- **By ID**: Using `find_element(By.ID, \"your_id\")` method.\n",
    "- **By Name**: Using `find_element(By.NAME, \"your_name\")` method.\n",
    "- **By Class Name**: Using `find_element(By.CLASS_NAME, \"your_class_name\")` method.\n",
    "- **By CSS Selector**: Using `find_element(By.CSS_SELECTOR, \"your_css_selector\")` method.\n",
    "- **By XPath**: Using `find_element(By.XPATH, \"your_xpath\")` method.\n",
    "\n",
    "For example, to extract the content of a paragraph with `id=\"content\"`, we can use:\n",
    "\n",
    "```python\n",
    "content = driver.find_element(By.ID, \"content\").text\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa95c80-451c-4981-96a4-789261de9304",
   "metadata": {},
   "source": [
    "## Handling Dynamic Content\n",
    "\n",
    "Dynamic websites may take some time to load content using JavaScript. When scraping dynamic content, we should wait for the elements to become visible before extracting data. We can achieve this using **Explicit Waits** provided by Selenium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbf1af-a488-4cec-81aa-b804e03ef385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "\n",
    "# List of URLs to scrape\n",
    "BASE_URL = [\n",
    "'http://www.hubertiming.com/results/2017GPTR10K'\n",
    "]\n",
    "board_members = []\n",
    "\n",
    "# Initialize ChromeDriver with automatic open.\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Loop through our URLs we loaded above\n",
    "for b in BASE_URL:\n",
    "    driver.get(b)\n",
    "\n",
    "    # Wait for the table to load\n",
    "    try:\n",
    "        officer_table = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, 'individualResults'))\n",
    "        )\n",
    "        \n",
    "#Find By Tag Name\n",
    "\n",
    "        # Loop through the rows of the table\n",
    "        for row in officer_table.find_elements(By.TAG_NAME, 'tr'):\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(cols) == 9:  # Ensure there are 9 columns\n",
    "                board_members.append(\n",
    "                    (cols[0].text.strip(),  # Place\n",
    "                     cols[1].text.strip(),  # Bib\n",
    "                     cols[2].text.strip(),  # Name\n",
    "                     cols[3].text.strip(),  # Gender\n",
    "                     cols[4].text.strip(),  # City\n",
    "                     cols[5].text.strip(),  # State\n",
    "                     cols[6].text.strip(),  # Time\n",
    "                     cols[7].text.strip(),  # Gun Time\n",
    "                     cols[8].text.strip())  # Team\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping {b}: {e}\")\n",
    "        continue  # Skip to the next URL if there's an error\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Define CSV file name\n",
    "csv_file = 'board_members.csv'\n",
    "\n",
    "# Save the scraped board members data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow(['Place', 'Bib', 'Name', 'Gender', 'City', 'State', 'Time', 'Gun Time', 'Team'])\n",
    "    \n",
    "    # Write data rows\n",
    "    writer.writerows(board_members)\n",
    "\n",
    "print(f\"Board members data saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e359e-c9a6-4040-9fea-eab634eb8e9d",
   "metadata": {},
   "source": [
    "## Handling User Interactions\n",
    "Some websites require user interactions (e.g., clicking buttons, filling forms) to load data dynamically. Selenium can simulate these interactions using methods like `‘click()’`, `‘send_keys()’`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ab8ee-c681-463f-9c63-fc03a3770c84",
   "metadata": {},
   "source": [
    "`search_input = driver.find_element(By.ID, 'search_input')` \n",
    "\n",
    "`search_input.send_keys(‘Web Scraping’)`\n",
    "\n",
    "`search_button = driver.find_element(By.ID, 'search_button') `\n",
    "\n",
    "`search_button.click()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a11b5c-6bec-4d31-a3a5-ce221c759ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Locate the search input field by ID\n",
    "search_input = driver.find_element(By.ID, 'search_input')\n",
    "\n",
    "# Simulate typing 'Web Scraping' into the input field\n",
    "search_input.send_keys('Web Scraping')\n",
    "\n",
    "# Locate the search button by ID\n",
    "search_button = driver.find_element(By.ID, 'search_button')\n",
    "\n",
    "# Simulate clicking the search button\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21be846-ce9f-4057-92b6-26f4bd903014",
   "metadata": {},
   "source": [
    "# Real-World Use Case: Scraping Product Data from an E-commerce Website\n",
    "\n",
    "To showcase Selenium’s full capabilities, let’s consider a more complex real-world use case. We will scrape product data from an e-commerce website.\n",
    "\n",
    "1. Open an e-commerce website (e.g., `https://www.example-ecommerce.com`).\n",
    "2. Search for a specific product category (e.g., \"Laptops\").\n",
    "3. Extract and print product names, prices, and ratings.\n",
    "\n",
    "## Executable Code Below:\n",
    "\n",
    "Below is a well-detailed Python code with comments to scrape product data from an e-commerce website using Selenium. For this example, we’ll scrape product data from Amazon’s \"Best Sellers in Electronics\" page using the concepts we have learned:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc5f82-2063-43f9-98ad-2441d1b20bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Automatically install and initialize ChromeDriver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Open Amazon’s Best Sellers in Electronics page\n",
    "driver.get('https://www.amazon.com/gp/bestsellers/electronics/')\n",
    "\n",
    "# Wait for the list of products to be visible\n",
    "product_list = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"zg-item-immersion\"]'))\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store product data\n",
    "product_data = []\n",
    "\n",
    "# Loop through each product element and extract data\n",
    "for product in product_list:\n",
    "    # Extract product name\n",
    "    product_name = product.find_element(By.XPATH, './/div[@class=\"p13n-sc-truncate p13n-sc-line-clamp-2\"]').text.strip()\n",
    "\n",
    "    # Extract product price (if available)\n",
    "    try:\n",
    "        product_price = product.find_element(By.XPATH, './/span[@class=\"p13n-sc-price\"]').text.strip()\n",
    "    except:\n",
    "        product_price = \"Price not available\"\n",
    "\n",
    "    # Extract product rating (if available)\n",
    "    try:\n",
    "        product_rating = product.find_element(By.XPATH, './/span[@class=\"a-icon-alt\"]').get_attribute(\"innerHTML\")\n",
    "    except:\n",
    "        product_rating = \"Rating not available\"\n",
    "\n",
    "    # Append the product data to the list\n",
    "    product_data.append({\n",
    "        'Product Name': product_name,\n",
    "        'Price': product_price,\n",
    "        'Rating': product_rating\n",
    "    })\n",
    "\n",
    "# Print the scraped product data\n",
    "print(\"Scraped Product Data:\")\n",
    "for idx, product in enumerate(product_data, start=1):\n",
    "    print(f\"{idx}. {product['Product Name']} — Price: {product['Price']}, Rating: {product['Rating']}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afead167-a4fa-412b-8b7a-d51b9ce51b77",
   "metadata": {},
   "source": [
    "# Explanation of the Code:\n",
    "\n",
    "1. **Import necessary modules** from Selenium to interact with the web page and locate elements.\n",
    "\n",
    "2. **Initialize the ChromeDriver** and open Amazon’s “Best Sellers in Electronics” page.\n",
    "\n",
    "3. **Use an 'Explicit Wait'** to wait for the list of products to be visible. This ensures that the web page has loaded, and the product elements are ready to be scraped.\n",
    "\n",
    "4. **Initialize an empty list, `product_data`**, to store the scraped product data.\n",
    "\n",
    "5. **Loop through each product element** and extract the product name, price, and rating (if available). We use `try-except` blocks to handle cases where the price or rating information is not available for a particular product.\n",
    "\n",
    "6. **Append the extracted product data** to the `product_data` list as a dictionary with keys ‘Product Name’, ‘Price’, and ‘Rating’.\n",
    "\n",
    "7. **After scraping all products**, print the scraped product data in a user-friendly format.\n",
    "\n",
    "8. Finally, **close the browser using `driver.quit()`**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169f9c3-8f11-400d-9bb1-3648355caa98",
   "metadata": {},
   "source": [
    "## Dealing with Anti-Scraping Measures\n",
    "\n",
    "Many websites implement anti-scraping measures to prevent automated data extraction. Techniques like IP blocking, CAPTCHAs, and user-agent detection can hinder scraping efforts. Understanding these measures and implementing strategies to bypass them responsibly is crucial for successful web scraping.\n",
    "\n",
    "## Data Storage and Management\n",
    "\n",
    "Once data is scraped, it needs to be stored and managed efficiently. Choosing the right data storage format (e.g., CSV, JSON, database) and organizing scraped data will facilitate further analysis and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fa477-5d55-4223-8979-a6a3c678879e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
