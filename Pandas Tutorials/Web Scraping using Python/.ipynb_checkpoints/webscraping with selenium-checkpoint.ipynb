{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0b81bc-aeb8-4f63-b619-e43b84ea4112",
   "metadata": {},
   "source": [
    "# Selenium Web Scraping\n",
    "\n",
    "Selenium web scraping involves using the browser automation tool to extract data from websites. Selenium allows you to programmatically control a web browser, meaning you can interact with websites just like a human user would. While various tools and libraries are available for web scraping in Python, Selenium stands out as a robust option, especially when dealing with websites that rely heavily on JavaScript for dynamic content rendering.\n",
    "\n",
    "## Overview of Selenium Web Scraping\n",
    "\n",
    "1. **Setup**: Install Selenium and a web driver (like ChromeDriver for Chrome or GeckoDriver for Firefox).\n",
    "2. **Automation**: Use Selenium to open a web browser and navigate to the target website.\n",
    "3. **Interaction**: Perform actions like clicking buttons, filling out forms, and scrolling, to interact with the web page.\n",
    "4. **Data Extraction**: Extract the desired data from the web page’s HTML content.\n",
    "5. **Processing**: Process and store the extracted data as needed.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, ensure that you have the following installed:\n",
    "\n",
    "1. **Python**: Download the latest version of Python from the official website.\n",
    "2. **Chrome Browser**: Selenium works best with the Chrome browser, so make sure you have it installed on your machine.\n",
    "3. **ChromeDriver**: ChromeDriver is essential for Selenium to control the Chrome browser. You can download it from the official website and ensure that the version matches your installed Chrome browser.\n",
    "4. **Selenium Library**: Install Selenium using pip with the following command:\n",
    "    \n",
    "   ` pip install selenium`\n",
    "    \n",
    "\n",
    "You also need to have:\n",
    "- Basic Python knowledge\n",
    "- Basic HTML\n",
    "- An open mind to learn new things\n",
    "\n",
    "## Getting Started with Selenium\n",
    "\n",
    "Let’s start with a basic example to understand how Selenium works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f70b958-4120-4969-a3ea-79ee5287a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->webdriver-manager) (2.1.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rmnjs\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604cede9-ff1e-496c-81aa-74d109d483f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Neo4j documentation - Neo4j Documentation\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize ChromeDriver with automatic open chrome browser installed in your pc\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a website\n",
    "driver.get('https://neo4j.com/docs/')\n",
    "\n",
    "# Extract the page title\n",
    "page_title = driver.title\n",
    "print(\"Page Title:\", page_title)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d59439-11af-4baa-9262-a4810bd6090a",
   "metadata": {},
   "source": [
    "In the above code, we imported the ‘webdriver’ module from Selenium, initialized the ChromeDriver, opened a website (https://www.example.com), extracted its page title, and then closed the browser using the ‘quit()’ method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bd5bc-1dd5-4082-9e96-5520b7902aa7",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium\n",
    "\n",
    "Now that we have a basic understanding of Selenium, let’s explore more advanced web scraping concepts. Many websites load data dynamically using JavaScript, which makes standard libraries like `requests` and `beautifulsoup` inadequate. Selenium’s ability to interact with JavaScript-rendered content makes it a powerful choice for such scenarios.\n",
    "\n",
    "## Locating Elements\n",
    "\n",
    "To scrape data effectively, we need to locate elements on the web page. Elements can be located using various methods:\n",
    "\n",
    "- **By ID**: Using `find_elements(By.ID, \"your_id\")` method.\n",
    "- **By Name**: Using `find_elements(By.NAME, \"your_name\")` method.\n",
    "- **By Class Name**: Using `find_elements(By.CLASS_NAME, \"your_class_name\")` method.\n",
    "- **By CSS Selector**: Using `find_elements(By.CSS_SELECTOR, \"your_css_selector\")` method.\n",
    "- **By XPath**: Using `find_elements(By.XPATH, \"your_xpath\")` method.\n",
    "\n",
    "For example, to extract the content of a paragraph with `id=\"content\"`, we can use:\n",
    "\n",
    "```python\n",
    "content = driver.find_elements(By.ID, \"content\").text\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0d930-9c9b-4902-9a35-0356724012da",
   "metadata": {},
   "source": [
    "## Handling Dynamic Content\n",
    "\n",
    "Dynamic websites may take some time to load content using JavaScript. When scraping dynamic content, we should wait for the elements to become visible before extracting data. We can achieve this using **Explicit Waits** provided by Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cee01-ca23-4b07-8d32-c98b6a1d1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "\n",
    "# List of URLs to scrape\n",
    "BASE_URL = [\n",
    "'http://www.hubertiming.com/results/2017GPTR10K'\n",
    "]\n",
    "board_members = []\n",
    "\n",
    "# Initialize ChromeDriver with automatic open.\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Loop through our URLs we loaded above\n",
    "for b in BASE_URL:\n",
    "    driver.get(b)\n",
    "\n",
    "    # Wait for the table to load\n",
    "    try:\n",
    "        officer_table = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, 'individualResults'))\n",
    "        )\n",
    "        \n",
    "#Find By Tag Name\n",
    "\n",
    "        # Loop through the rows of the table\n",
    "        for row in officer_table.find_elements(By.TAG_NAME, 'tr'):\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(cols) == 9:  # Ensure there are 9 columns\n",
    "                board_members.append(\n",
    "                    (cols[0].text.strip(),  # Place\n",
    "                     cols[1].text.strip(),  # Bib\n",
    "                     cols[2].text.strip(),  # Name\n",
    "                     cols[3].text.strip(),  # Gender\n",
    "                     cols[4].text.strip(),  # City\n",
    "                     cols[5].text.strip(),  # State\n",
    "                     cols[6].text.strip(),  # Time\n",
    "                     cols[7].text.strip(),  # Gun Time\n",
    "                     cols[8].text.strip())  # Team\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping {b}: {e}\")\n",
    "        continue  # Skip to the next URL if there's an error\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Define CSV file name\n",
    "csv_file = 'board_members.csv'\n",
    "\n",
    "# Save the scraped board members data to a CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow(['Place', 'Bib', 'Name', 'Gender', 'City', 'State', 'Time', 'Gun Time', 'Team'])\n",
    "    \n",
    "    # Write data rows\n",
    "    writer.writerows(board_members)\n",
    "\n",
    "print(f\"Board members data saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d6c76-5b53-4c5f-bd4d-753b6eda56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find by class name, XPATH name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c206295d-72f8-46c6-9beb-4ac2de7e34f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j documentation\n",
      "Deployment options\n",
      "Choose from fully and self-managed local and cloud deployments. Run Neo4j on Docker or Kubernetes.\n",
      "Get a Neo4j instance\n",
      "Cypher\n",
      "Learn how to write Cypher®, Neo4j’s declarative query language.\n",
      "Query your data\n",
      "Neo4j Tools\n",
      "Use Neo4j’s tools to explore, visualize, manage, monitor, and import data to your graph.\n",
      "Discover the products\n",
      "Graph Data Science\n",
      "Run graph algorithms and machine learning models to analyze your data at scale.\n",
      "Get insights from data\n",
      "Create applications\n",
      "Discover the client libraries and APIs to develop applications with Neo4j and AuraDB.\n",
      "Start developing\n",
      "Connect data sources\n",
      "Learn how to use connectors and other tools to connect Neo4j with other data sources.\n",
      "Connect to Neo4j\n",
      "Keep exploring\n",
      "Developer\n",
      "Choose your deployment\n",
      "Learn Cypher\n",
      "Start querying\n",
      "Create applications\n",
      "Connect data sources\n",
      "Integrate GenAI functions\n",
      "Improve app performance\n",
      "Extend Neo4j\n",
      "Database Admin\n",
      "Manage your database\n",
      "Deploy and manage a cluster\n",
      "Database internals\n",
      "Manage users, roles, and privileges\n",
      "Monitor your database\n",
      "Backup and restore\n",
      "Upgrade and migration\n",
      "Data Scientist\n",
      "Analyze your data\n",
      "Set up your deployment\n",
      "Create data visualizations\n",
      "Use machine learning algorithms\n",
      "Set up a Python client\n",
      "Manage your graph\n",
      "Data Engineer\n",
      "Graph database concepts\n",
      "Model your data\n",
      "Connect data sources\n",
      "Import your dataset\n",
      "Monitor data changes\n",
      "Data modeling tools\n",
      "Tutorials & How-to guides\n",
      "Embedding & Vector Indexes Tutorial\n",
      "Import data from a relational database into Neo4j\n",
      "Build a Cypher recommendation engine\n",
      "Set up and use a composite database\n",
      "Capture and track changes in real-time\n",
      "Apply centrality algorithms to your graph\n",
      "All tutorials\n",
      "Learn and become Neo4j certified\n",
      "Explore GraphAcademy\n",
      "Join forums and discussions\n",
      "Community forum Discord\n",
      "Developer blogs, articles and books\n",
      "Developer blog Other resources\n",
      "https://neo4j.com/docs/upgrade-migration-guide/current\n",
      "https://neo4j.com/docs/graph-data-science/current/getting-started\n",
      "https://neo4j.com/docs/operations-manual/current/authentication-authorization\n",
      "https://neo4j.com/docs/model\n",
      "https://neo4j.com/docs/operations-manual/current/database-administration\n",
      "https://neo4j.com/docs/tutorials\n",
      "https://neo4j.com/docs/import\n",
      "https://neo4j.com/docs/graph-data-science/current/machine-learning/machine-learning\n",
      "https://neo4j.com/docs/cypher-manual/current/queries\n",
      "https://neo4j.com/docs/graph-data-science/current/management-ops\n",
      "https://neo4j.com/docs/getting-started/appendix/graphdb-concepts\n",
      "https://neo4j.com/docs/cypher-manual/current/planning-and-tuning/query-tuning\n",
      "https://neo4j.com/docs/genai\n",
      "https://neo4j.com/docs/cdc/current\n",
      "https://discord.com/invite/neo4j\n",
      "https://community.neo4j.com\n",
      "https://neo4j.com/docs/getting-started/cypher-intro\n",
      "https://neo4j.com/docs/graph-data-science/current/production-deployment\n",
      "https://neo4j.com/docs/create-applications\n",
      "https://neo4j.com/docs/graph-data-science-client/current\n",
      "https://neo4j.com/docs/getting-started/appendix/tutorials/guide-import-relational-and-etl\n",
      "https://neo4j.com/docs/cdc/current/procedures/query-examples\n",
      "https://neo4j.com/docs/operations-manual/current/tutorial/tutorial-composite-database\n",
      "https://neo4j.com/docs/java-reference\n",
      "https://neo4j.com/docs/cypher\n",
      "https://neo4j.com/docs/reference/resources\n",
      "https://neo4j.com/docs/operations-manual/current/database-internals\n",
      "https://neo4j.com/docs/connectors\n",
      "https://neo4j.com/docs/operations-manual/current/monitoring\n",
      "https://neo4j.com/docs/getting-started/data-modeling/data-modeling-tools\n",
      "https://neo4j.com/docs/operations-manual/current/backup-restore\n",
      "https://neo4j.com/docs\n",
      "https://neo4j.com/docs/getting-started/appendix/tutorials/guide-build-a-recommendation-engine\n",
      "https://neo4j.com/docs/graph-data-science-client/current/tutorials/centrality-algorithms\n",
      "https://neo4j.com/docs/genai/tutorials/embeddings-vector-indexes\n",
      "https://graphacademy.neo4j.com\n",
      "https://neo4j.com/docs/gds\n",
      "https://neo4j.com/developer-blog\n",
      "https://neo4j.com/docs/operations-manual/current/clustering\n",
      "https://neo4j.com/docs/tools\n",
      "https://neo4j.com/docs/deployment-options\n",
      "https://neo4j.com/docs/bloom-user-guide/current/bloom-tutorial/gds-integration\n"
     ]
    }
   ],
   "source": [
    "#???????????????????????????Required Packages??????????????????????????????\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "# ??????????????????????End of Required Packages???????????????????????????????????\n",
    "\n",
    "\n",
    "# ?????????????????????? function defination Start ??????????????????????????????\n",
    "def extract_text_by_class(c):\n",
    "    global driver\n",
    "    try:\n",
    "        content = driver.find_element(By.CLASS_NAME, c)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global driver\n",
    "    links = set()\n",
    "    try:\n",
    "        xpath_elems = driver.find_elements(By.XPATH, xpath)\n",
    "        for elem in xpath_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ????????????? Function defination End ???????????????????????????????????????????????\n",
    "\n",
    "# driver to run run webpage\n",
    "driver=webdriver.Chrome()\n",
    "# URL of the website you want to scrape\n",
    "\n",
    "#url = input(\"Enter URL of the website you want to scrape: \\n \")\n",
    "url = 'https://neo4j.com/docs'\n",
    "\n",
    "# Open the website in the browser\n",
    "driver.get(url)\n",
    "\n",
    "# Extract text from the content div\n",
    "text = extract_text_by_class(\"content\")\n",
    "    # If nothing is found, try article div\n",
    "if not text:\n",
    "    text = extract_text_by_class(\"article\")\n",
    "    # If nothing is found, try page div\n",
    "if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "# Check if 404\n",
    "try:\n",
    "     if \"Sorry, page not found\" in driver.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "except:\n",
    "        pass\n",
    "\n",
    "print(text)\n",
    "\n",
    "   # Extract links from the content div\n",
    "links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    # If nothing is found, try article div\n",
    "if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "#print(links)\n",
    "for link in links:\n",
    "   print(link) \n",
    "\n",
    "#Closes the current browser window that the WebDriver is controlling.\n",
    "#Useful when you have multiple browser windows or tabs open and you want to close a specific one.\n",
    "#If only one browser window is open, it will close that window but the WebDriver session will still be active.\n",
    "driver.close() \n",
    "\n",
    "#Closes all browser windows and ends the WebDriver session.\n",
    "#Useful when you are done with the entire browser session and want to clean up all resources.\n",
    "#Ends the WebDriver session completely, closing all associated browser windows.\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a6845-414c-4c8b-9266-74265be7f1bf",
   "metadata": {},
   "source": [
    "## Handling User Interactions\n",
    "Some websites require user interactions (e.g., clicking buttons, filling forms) to load data dynamically. Selenium can simulate these interactions using methods like `‘click()’`, `‘send_keys()’`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa604113-c4d9-47ec-949e-b7c15ea30034",
   "metadata": {},
   "source": [
    "`search_input = driver.find_elements(By.ID, 'search_input')` \n",
    "\n",
    "`search_input.send_keys(‘Web Scraping’)`\n",
    "\n",
    "`search_button = driver.find_elements(By.ID, 'search_button') `\n",
    "\n",
    "`search_button.click()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937501d6-d8f9-431f-a8b1-eb7d19ff8a15",
   "metadata": {},
   "source": [
    "### this is a demo example for Automating LinkedIn using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2b11c6-9908-4221-96a1-ef5b7990c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.linkedin.com/checkpoint/lg/sign-in-another-account?trk=guest_homepage-basic_nav-header-signin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c2917d-17f8-4ef6-b700-ccbfa27a04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = browser.find_element(By.ID,'username')\n",
    "email.send_keys('demo@gmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c988ad3-5a13-4e94-8cb0-6de041aeb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "password=browser.find_element(By.ID,'password')\n",
    "password.send_keys('*******')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea3c3c0-2214-4d40-a9b9-60fbbf66f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "password.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efe51e9-a99a-407c-8e44-b00b781b7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e6378-5bc8-4e45-b32c-8bf141d6060d",
   "metadata": {},
   "source": [
    "example: Ticket Booking Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbba7a0-18b5-4739-9b91-4e9a1a4766ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
